version: "3.8"

name: "ai"

services:
  llm:
    container_name: llm
    build:
      dockerfile: ./Dockerfile.llm
      args:
        MODEL_URL: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf?download=true
    ports:
      - "8081:8081"
    environment:
      - LLAMA_EXECUTABLE_PATH=/bin/llama/server
      - LLAMA_MODEL_PATH=/models/model
    networks:
      ai:

  ai:
    container_name: ai
    build:
      dockerfile: ./Dockerfile.ai
    ports:
      - "8080:8080"
    environment:
      - SQLITE_PATH=/data/ai.db
      - JWT_SECRET=secret
      - LLM_ADDRESS=llm:8081
    volumes:
      - ai_data:/data
    networks:
      ai:

  envoy:
    container_name: envoy
    build:
      dockerfile: ./Dockerfile.envoy
    ports:
      - "9090:8080"
    networks:
      ai:

  ui:
    container_name: ui
    build:
      dockerfile: ./Dockerfile.ui
      args:
        AI_HOST: localhost
        AI_PORT: 8080
        AI_WEB_PORT: 9090
        AI_SECURE: false
    ports:
      - "8090:80"
    networks:
      ai:

volumes:
  ai_data:

networks:
  ai:
